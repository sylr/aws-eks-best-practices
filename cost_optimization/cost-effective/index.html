
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.1.2, mkdocs-material-7.1.2">
    
    
      
        <title>Cost-effective resources - EKS Best Practices Guides</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.6f955dcd.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.ef6f36e2.min.css">
        
      
    
    
    
      
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700%7CRoboto+Mono&display=fallback">
        <style>:root{--md-text-font-family:"Roboto";--md-code-font-family:"Roboto Mono"}</style>
      
    
    
    
    
      
    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="" data-md-color-primary="none" data-md-color-accent="none">
  
    
    <script>function __prefix(e){return new URL("../..",location).pathname+"."+e}function __get(e,t=localStorage){return JSON.parse(t.getItem(__prefix(e)))}</script>
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#cost-effective-resources" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
      <header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="EKS Best Practices Guides" class="md-header__button md-logo" aria-label="EKS Best Practices Guides" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            EKS Best Practices Guides
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Cost-effective resources
            
          </span>
        </div>
      </div>
    </div>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
      </label>
      
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" data-md-state="active" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
      </label>
      <button type="reset" class="md-search__icon md-icon" aria-label="Clear" tabindex="-1">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41z"/></svg>
      </button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
      <div class="md-header__source">
        
<a href="https://github.com/aws/aws-eks-best-practices/" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    aws/aws-eks-best-practices
  </div>
</a>
      </div>
    
  </nav>
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="EKS Best Practices Guides" class="md-nav__button md-logo" aria-label="EKS Best Practices Guides" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54z"/></svg>

    </a>
    EKS Best Practices Guides
  </label>
  
    <div class="md-nav__source">
      
<a href="https://github.com/aws/aws-eks-best-practices/" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    aws/aws-eks-best-practices
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_1" type="checkbox" id="__nav_1" >
      
      <label class="md-nav__link" for="__nav_1">
        Guides
        <span class="md-nav__icon md-icon"></span>
      </label>
      <nav class="md-nav" aria-label="Guides" data-md-level="1">
        <label class="md-nav__title" for="__nav_1">
          <span class="md-nav__icon md-icon"></span>
          Guides
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        Introduction
      </a>
    </li>
  

          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_2" type="checkbox" id="__nav_2" >
      
      <label class="md-nav__link" for="__nav_2">
        Security
        <span class="md-nav__icon md-icon"></span>
      </label>
      <nav class="md-nav" aria-label="Security" data-md-level="1">
        <label class="md-nav__title" for="__nav_2">
          <span class="md-nav__icon md-icon"></span>
          Security
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
  
  
  
    <li class="md-nav__item">
      <a href="../../security/docs/" class="md-nav__link">
        Home
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../../security/docs/iam/" class="md-nav__link">
        Identity and Access Management
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../../security/docs/pods/" class="md-nav__link">
        Pod Security
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../../security/docs/multitenancy/" class="md-nav__link">
        Multi-tenancy
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../../security/docs/detective/" class="md-nav__link">
        Detective Controls
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../../security/docs/network/" class="md-nav__link">
        Network Security
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../../security/docs/data/" class="md-nav__link">
        Data Encryption and Secrets Management
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../../security/docs/runtime/" class="md-nav__link">
        Runtime Security
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../../security/docs/hosts/" class="md-nav__link">
        Infrastructure Security
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../../security/docs/compliance/" class="md-nav__link">
        Regulatory Compliance
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../../security/docs/incidents/" class="md-nav__link">
        Incident Response and Forensics
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../../security/docs/image/" class="md-nav__link">
        Image Security
      </a>
    </li>
  

          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3" type="checkbox" id="__nav_3" >
      
      <label class="md-nav__link" for="__nav_3">
        Cluster Autoscaling
        <span class="md-nav__icon md-icon"></span>
      </label>
      <nav class="md-nav" aria-label="Cluster Autoscaling" data-md-level="1">
        <label class="md-nav__title" for="__nav_3">
          <span class="md-nav__icon md-icon"></span>
          Cluster Autoscaling
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
  
  
  
    <li class="md-nav__item">
      <a href="../../cluster-autoscaling/cluster-autoscaling/" class="md-nav__link">
        Home
      </a>
    </li>
  

          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_4" type="checkbox" id="__nav_4" >
      
      <label class="md-nav__link" for="__nav_4">
        Reliability
        <span class="md-nav__icon md-icon"></span>
      </label>
      <nav class="md-nav" aria-label="Reliability" data-md-level="1">
        <label class="md-nav__title" for="__nav_4">
          <span class="md-nav__icon md-icon"></span>
          Reliability
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
  
  
  
    <li class="md-nav__item">
      <a href="../../reliability/docs/" class="md-nav__link">
        Home
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../../reliability/docs/application/" class="md-nav__link">
        Applications
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../../reliability/docs/controlplane/" class="md-nav__link">
        Control Plane
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../../reliability/docs/dataplane/" class="md-nav__link">
        Data Plane
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../../reliability/docs/networkmanagement/" class="md-nav__link">
        Network
      </a>
    </li>
  

          
        </ul>
      </nav>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#recommendations" class="md-nav__link">
    Recommendations
  </a>
  
    <nav class="md-nav" aria-label="Recommendations">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#ensure-that-the-infrastructure-used-to-deploy-the-containerized-service-matches-the-application-profile-and-scaling-needs" class="md-nav__link">
    Ensure that the infrastructure used to deploy the containerized service matches the application profile and scaling needs
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#use-cluster-autoscaler-to-adjust-the-size-of-a-kubernetes-cluster-to-meet-the-current-needs" class="md-nav__link">
    Use Cluster Autoscaler to adjust the size of a Kubernetes cluster to meet the current needs
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#deploy-horizontal-pod-autoscaling-to-automatically-scales-the-number-of-pods-in-a-deployment-replication-controller-or-replica-set-based-on-that-resources-cpu-utilization-or-other-application-related-metrics" class="md-nav__link">
    Deploy Horizontal Pod Autoscaling to automatically scales the number of pods in a deployment, replication controller, or replica set based on that resource's CPU utilization or other application related metrics
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#recommendations_1" class="md-nav__link">
    Recommendations
  </a>
  
    <nav class="md-nav" aria-label="Recommendations">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#use-down-scaling-to-scale-down-kubernetes-deployments-statefulsets-andor-horizontalpodautoscalers-during-non-work-hours" class="md-nav__link">
    Use Down Scaling to scale down Kubernetes Deployments, StatefulSets, and/or HorizontalPodAutoscalers during non-work hours.
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#use-limitranges-and-resource-quotas-to-help-manage-costs-by-constraining-the-amount-of-resources-allocated-at-an-namespace-level" class="md-nav__link">
    Use LimitRanges and Resource Quotas to help manage costs by constraining the amount of resources allocated at an Namespace level
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#use-pricing-models-for-effective-utilization" class="md-nav__link">
    Use pricing models for effective utilization
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#use-spot-ec2-instances" class="md-nav__link">
    Use Spot EC2 Instances:
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#use-compute-savings-plan" class="md-nav__link">
    Use Compute Savings Plan
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#resources" class="md-nav__link">
    Resources
  </a>
  
    <nav class="md-nav" aria-label="Resources">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#videos" class="md-nav__link">
    Videos
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#documentation-and-blogs" class="md-nav__link">
    Documentation and Blogs
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#tools" class="md-nav__link">
    Tools
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content" data-md-component="content">
            <article class="md-content__inner md-typeset">
              
                
                  <a href="https://github.com/aws/aws-eks-best-practices/edit/master/docs/cost_optimization/cost-effective.md" title="Edit this page" class="md-content__button md-icon">
                    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20.71 7.04c.39-.39.39-1.04 0-1.41l-2.34-2.34c-.37-.39-1.02-.39-1.41 0l-1.84 1.83 3.75 3.75M3 17.25V21h3.75L17.81 9.93l-3.75-3.75L3 17.25z"/></svg>
                  </a>
                
                
                <h1 id="cost-effective-resources">Cost-effective resources<a class="headerlink" href="#cost-effective-resources" title="Permanent link">&para;</a></h1>
<p>Cost Effective resources means using the appropriate services, resources, and configurations for your workloads running on a Kubernetes cluster, which will result in cost savings.</p>
<h2 id="recommendations">Recommendations<a class="headerlink" href="#recommendations" title="Permanent link">&para;</a></h2>
<h3 id="ensure-that-the-infrastructure-used-to-deploy-the-containerized-service-matches-the-application-profile-and-scaling-needs">Ensure that the infrastructure used to deploy the containerized service matches the application profile and scaling needs<a class="headerlink" href="#ensure-that-the-infrastructure-used-to-deploy-the-containerized-service-matches-the-application-profile-and-scaling-needs" title="Permanent link">&para;</a></h3>
<p>There are several types of Kubernetes autoscaling supported in Amazon EKS - <a href="https://docs.aws.amazon.com/eks/latest/userguide/cluster-autoscaler.html">Cluster Autoscaler</a>, <a href="https://docs.aws.amazon.com/eks/latest/userguide/horizontal-pod-autoscaler.html">Horizontal Pod Autoscaler</a> and <a href="https://docs.aws.amazon.com/eks/latest/userguide/vertical-pod-autoscaler.html">Vertical Pod Autoscaler</a>. This section covers two of them, Cluster Auto Scaler and Horizontal Pod Autoscaler.</p>
<h3 id="use-cluster-autoscaler-to-adjust-the-size-of-a-kubernetes-cluster-to-meet-the-current-needs">Use Cluster Autoscaler to adjust the size of a Kubernetes cluster to meet the current needs<a class="headerlink" href="#use-cluster-autoscaler-to-adjust-the-size-of-a-kubernetes-cluster-to-meet-the-current-needs" title="Permanent link">&para;</a></h3>
<p>The <a href="https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler">Kubernetes Cluster Autoscaler</a> automatically adjusts the number of nodes in the EKS cluster when pods fail to launch due to lack of resources or when nodes in the cluster are underutilized and their pods can be rescheduled onto other nodes in the cluster. The Cluster Autoscaler scales worker nodes within any specified Auto Scaling group and runs as a deployment in your EKS cluster.</p>
<p>Amazon EKS with EC2 managed node groups automate the provisioning and lifecycle management of nodes (Amazon EC2 instances) for Amazon EKS Kubernetes clusters. All managed nodes are provisioned as part of an Amazon EC2 Auto Scaling group that is managed for you by Amazon EKS and all resources including Amazon EC2 instances and Auto Scaling groups run within your AWS account. Amazon EKS tags managed node group resources so that they can be discovered the Kubernetes Cluster Autoscaler. </p>
<p>The documentation at https://docs.aws.amazon.com/eks/latest/userguide/cluster-autoscaler.html provides detailed guidance on setting up a Managed Node Group and then deploying Kubernetes Cluster Auto Scaler. If you are running a stateful application across multiple Availability Zones that is backed by Amazon EBS volumes and using the Kubernetes Cluster Autoscaler, you should configure multiple node groups, each scoped to a single Availability Zone.</p>
<p><em>Cluster Autoscaler logs for EC2 based Worker Nodes -</em>
<img alt="Kubernetes Cluster Auto Scaler logs" src="../../images/cluster-auto-scaler.png" /></p>
<p>When a pod cannot be scheduled due to lack of available resources, Cluster Autoscaler determines that the cluster must scale out and increases the size of the node group. When multiple node groups are used, Cluster Autoscaler chooses one based on the Expander configuration. Currently, the following strategies are supported in EKS: 
+ <strong>random</strong> - default expander, selects the instance group randomly
+ <strong>most-pods</strong> - selects the instance group that schedules the most amount of pods.
+ <strong>least-waste</strong> - selects the node group that will have the least idle CPU (if tied, unused memory) after scale-up. This is useful when you have different classes of nodes, for example, high CPU or high memory nodes, and only want to expand those when there are pending pods that need a lot of those resources.
+ <strong>priority</strong> - selects the node group that has the highest priority assigned by the user</p>
<p>You can use the <strong>random</strong> placement strategy for the Expander in Cluster Autoscaler, if EC2 Spot instances are being used as worker nodes. This is the default expander, and arbitrarily chooses a node-group when the cluster must scale out. The random expander maximizes your ability to leverage multiple Spot capacity pools. </p>
<p><a href="https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/expander/priority/readme.md"><strong>Priority</strong></a> based expander selects an expansion option based on priorities assigned by a user to scaling groups. Sample priorities can be to let Autoscaler first try to scale out a spot instance node group and then, if it cannot, falls back to scaling out an on-demand node group. </p>
<p><strong>most-pods</strong> based expander is useful when you are using nodeSelector to make sure certain pods land on certain nodes. </p>
<p>From the <a href="https://docs.aws.amazon.com/eks/latest/userguide/cluster-autoscaler.html">documentation</a> to specify <strong>least-waste</strong> as the expander type for the Cluster Autoscaling configuration:</p>
<div class="codehilite"><pre><span></span><code>    spec:
      containers:
      - command:
        - ./cluster-autoscaler
        - --v=4
        - --stderrthreshold=info
        - --cloud-provider=aws
        - --skip-nodes-with-local-storage=false
        - --expander=least-waste
        - --node-group-auto-discovery=asg:tag=k8s.io/cluster-autoscaler/enabled,k8s.io/cluster-autoscaler/&lt;YOUR CLUSTER NAME&gt;
        - --balance-similar-node-groups
        - --skip-nodes-with-system-pods=false
</code></pre></div>

<h3 id="deploy-horizontal-pod-autoscaling-to-automatically-scales-the-number-of-pods-in-a-deployment-replication-controller-or-replica-set-based-on-that-resources-cpu-utilization-or-other-application-related-metrics">Deploy Horizontal Pod Autoscaling to automatically scales the number of pods in a deployment, replication controller, or replica set based on that resource's CPU utilization or other application related metrics<a class="headerlink" href="#deploy-horizontal-pod-autoscaling-to-automatically-scales-the-number-of-pods-in-a-deployment-replication-controller-or-replica-set-based-on-that-resources-cpu-utilization-or-other-application-related-metrics" title="Permanent link">&para;</a></h3>
<p>The <a href="https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/">Kubernetes Horizontal Pod Autoscaler</a> automatically scales the number of pods in a deployment, replication controller, or replica set based on resource metrics like CPU utilization or with custom metrics support, on some other application-provided metrics. This can help your applications scale out to meet increased demand or scale in when resources are not needed, thus freeing up your worker nodes for other applications. When you set a target metric utilization percentage, the Horizontal Pod Autoscaler scales your application in or out to try to meet that target. </p>
<p>The <a href="https://github.com/awslabs/k8s-cloudwatch-adapter">k8s-cloudwatch-adapter</a> is an implementation of the Kubernetes Custom Metrics API and External Metrics API with integration for CloudWatch metrics. It allows you to scale your Kubernetes deployment using the Horizontal Pod Autoscaler (HPA) with CloudWatch metrics.</p>
<p>For an example of scaling using a resource metric like CPU, follow https://eksworkshop.com/beginner/080_scaling/test_hpa/ to deploy a sample app, perform a simple load test to test the autoscaling of pod and simulate pod autoscaling. </p>
<p>Refer to this <a href="https://aws.amazon.com/blogs/compute/scaling-kubernetes-deployments-with-amazon-cloudwatch-metrics/">blog</a> for an example of a custom metric for an application to scale according to the number of messages in the Amazon SQS (Simple Queue Service) queue.</p>
<p>An example of an external metric from Amazon SQS from the blog:</p>
<div class="codehilite"><pre><span></span><code><span class="nt">apiVersion</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">metrics.aws/v1alpha1</span>
<span class="nt">kind</span><span class="p">:</span> <span class="nt">ExternalMetric</span><span class="p">:</span>
  <span class="nt">metadata</span><span class="p">:</span>
    <span class="nt">name</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">hello-queue-length</span>
  <span class="nt">spec</span><span class="p">:</span>
    <span class="nt">name</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">hello-queue-length</span>
    <span class="nt">resource</span><span class="p">:</span>
      <span class="nt">resource</span><span class="p">:</span> <span class="s">&quot;deployment&quot;</span>
    <span class="nt">queries</span><span class="p">:</span>
      <span class="p p-Indicator">-</span> <span class="nt">id</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">sqs_helloworld</span>
        <span class="nt">metricStat</span><span class="p">:</span>
          <span class="nt">metric</span><span class="p">:</span>
            <span class="nt">namespace</span><span class="p">:</span> <span class="s">&quot;AWS/SQS&quot;</span>
            <span class="nt">metricName</span><span class="p">:</span> <span class="s">&quot;ApproximateNumberOfMessagesVisible&quot;</span>
            <span class="nt">dimensions</span><span class="p">:</span>
              <span class="p p-Indicator">-</span> <span class="nt">name</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">QueueName</span>
                <span class="nt">value</span><span class="p">:</span> <span class="s">&quot;helloworld&quot;</span>
          <span class="nt">period</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">300</span>
          <span class="nt">stat</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">Average</span>
          <span class="nt">unit</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">Count</span>
        <span class="nt">returnData</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">true</span>
</code></pre></div>

<p>An example of an HPA utilizing this external metric: </p>
<div class="codehilite"><pre><span></span><code><span class="nt">kind</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">HorizontalPodAutoscaler</span>
<span class="nt">apiVersion</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">autoscaling/v2beta1</span>
<span class="nt">metadata</span><span class="p">:</span>
  <span class="nt">name</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">sqs-consumer-scaler</span>
<span class="nt">spec</span><span class="p">:</span>
  <span class="nt">scaleTargetRef</span><span class="p">:</span>
    <span class="nt">apiVersion</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">apps/v1beta1</span>
    <span class="nt">kind</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">Deployment</span>
    <span class="nt">name</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">sqs-consumer</span>
  <span class="nt">minReplicas</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">1</span>
  <span class="nt">maxReplicas</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">10</span>
  <span class="nt">metrics</span><span class="p">:</span>
  <span class="p p-Indicator">-</span> <span class="nt">type</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">External</span>
    <span class="nt">external</span><span class="p">:</span>
      <span class="nt">metricName</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">hello-queue-length</span>
      <span class="nt">targetAverageValue</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">30</span>
</code></pre></div>

<p>The combination of Cluster Auto Scaler for the Kubernetes worker nodes and Horizontal Pod Autoscaler for the pods, will ensure that the provisioned resources will be as close to the actual utilization as possible.</p>
<p><img alt="Kubernetes Cluster AutoScaler and HPA" src="../../images/ClusterAS-HPA.png" />
<strong><em>(Image source: https://aws.amazon.com/blogs/containers/cost-optimization-for-kubernetes-on-aws/)</em></strong></p>
<p><strong><em>Amazon EKS with Fargate</em></strong></p>
<p><strong><em>*Horizontal Pod Autoscaling of Pods</em></strong>*</p>
<p>Autoscaling EKS on Fargate can be done using the following mechanisms:</p>
<ol>
<li>Using the Kubernetes metrics server and configure auto-scaling based on CPU and/or memory usage.</li>
<li>Configure autoscaling based on custom metrics like HTTP traffic using Prometheus and Prometheus metrics adapter</li>
<li>Configure autoscaling based on App Mesh traffic</li>
</ol>
<p>The above scenarios are explained in a hands-on blog on <a href="https://aws.amazon.com/blogs/containers/autoscaling-eks-on-fargate-with-custom-metrics/">"Autoscaling EKS on Fargate with custom metrics</a></p>
<p><strong><em>*Vertical Pod Autoscaling</em></strong>*</p>
<p>Use the <a href="https://docs.aws.amazon.com/eks/latest/userguide/vertical-pod-autoscaler.html">Vertical Pod Autoscaler</a> with pods running on Fargate to optimize the CPU and memory used for your applications. However, because changing the resource allocation for a pod requires the pod to be restarted, you must set the pod update policy to either Auto or Recreate to ensure correct functionality. </p>
<h2 id="recommendations_1">Recommendations<a class="headerlink" href="#recommendations_1" title="Permanent link">&para;</a></h2>
<h3 id="use-down-scaling-to-scale-down-kubernetes-deployments-statefulsets-andor-horizontalpodautoscalers-during-non-work-hours">Use Down Scaling to scale down Kubernetes Deployments, StatefulSets, and/or HorizontalPodAutoscalers during non-work hours.<a class="headerlink" href="#use-down-scaling-to-scale-down-kubernetes-deployments-statefulsets-andor-horizontalpodautoscalers-during-non-work-hours" title="Permanent link">&para;</a></h3>
<p>As part of controlling costs Down-Scaling resources that are not in-use can also have an huge impact on the overall costs. There are tools like <a href="https://github.com/hjacobs/kube-downscaler">kube-downscaler</a> and <a href="https://github.com/kubernetes-sigs/descheduler">Descheduler for Kubernetes</a>. </p>
<p><strong>Kube-descaler</strong>, can be used to Scale down Kubernetes deployments after work hours or during set periods of time. </p>
<p><strong>Descheduler for Kubernetes</strong>, based on its policy, can find pods that can be moved and evicts them.  In its current implementation, the kubernetes descheduler does not reschedule evicted pods but relies on the default scheduler for that</p>
<p><strong>Kube-descaler</strong></p>
<p><em>Installation of kube-downscaler</em>:</p>
<div class="codehilite"><pre><span></span><code>git clone https://github.com/hjacobs/kube-downscaler
cd kube-downscaler
kubectl apply -k deploy/
</code></pre></div>

<p>The example configuration uses the --dry-run as a safety flag to prevent downscaling --- remove it to enable the downscaler, e.g. by editing the deployment:</p>
<div class="codehilite"><pre><span></span><code>$ kubectl edit deploy kube-downscaler
</code></pre></div>

<p>Deploy an nginx pod and schedule it to be run in the time zone - Mon-Fri 09:00-17:00 Asia/Kolkata: </p>
<div class="codehilite"><pre><span></span><code>$ kubectl run nginx1 --image<span class="o">=</span>nginx
$ kubectl annotate deploy nginx1 <span class="s1">&#39;downscaler/uptime=Mon-Fri 09:00-17:00 Asia/Kolkata&#39;</span>
</code></pre></div>

<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The default grace period of 15 minutes applies to the new nginx deployment, i.e. if the current time is not within Mon-Fri 9-17 (Asia/Kolkata timezone), it will downscale not immediately, but after 15 minutes. </p>
</div>
<p><img alt="Kube-down-scaler for nginx" src="../../images/kube-down-scaler.png" /></p>
<p>More advanced downscaling deployment scenarios are available at the <a href="https://github.com/hjacobs/kube-downscaler">kube-down-scaler github project</a>.</p>
<p><strong>Kubernetes descheduler</strong></p>
<p>The descheduler can be run as a Job or CronJob inside of a k8s cluster. Descheduler's policy is configurable and includes strategies that can be enabled or disabled. Seven strategies <em>RemoveDuplicates</em>, <em>LowNodeUtilization</em>, <em>RemovePodsViolatingInterPodAntiAffinity</em>, <em>RemovePodsViolatingNodeAffinity</em>, <em>RemovePodsViolatingNodeTaints</em>, <em>RemovePodsHavingTooManyRestarts</em>, and <em>PodLifeTime</em> are currently implemented. More details can be found in their <a href="https://github.com/kubernetes-sigs/descheduler">documentation</a>.</p>
<p>A sample policy, which has the descheduler enabled for lowcpuutilization of nodes (where it covers the scenarios for both underutilized and overutilized), removing pods for too many restarts and others :</p>
<div class="codehilite"><pre><span></span><code><span class="nt">apiVersion</span><span class="p">:</span> <span class="s">&quot;descheduler/v1alpha1&quot;</span>
<span class="nt">kind</span><span class="p">:</span> <span class="s">&quot;DeschedulerPolicy&quot;</span>
<span class="nt">strategies</span><span class="p">:</span>
  <span class="s">&quot;RemoveDuplicates&quot;</span><span class="p p-Indicator">:</span>
     <span class="nt">enabled</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">true</span>
  <span class="s">&quot;RemovePodsViolatingInterPodAntiAffinity&quot;</span><span class="p p-Indicator">:</span>
     <span class="nt">enabled</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">true</span>
  <span class="s">&quot;LowNodeUtilization&quot;</span><span class="p p-Indicator">:</span>
     <span class="nt">enabled</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">true</span>
     <span class="nt">params</span><span class="p">:</span>
       <span class="nt">nodeResourceUtilizationThresholds</span><span class="p">:</span>
         <span class="nt">thresholds</span><span class="p">:</span>
           <span class="s">&quot;cpu&quot;</span> <span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">20</span>
           <span class="s">&quot;memory&quot;</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">20</span>
           <span class="s">&quot;pods&quot;</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">20</span>
         <span class="nt">targetThresholds</span><span class="p">:</span>
           <span class="s">&quot;cpu&quot;</span> <span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">50</span>
           <span class="s">&quot;memory&quot;</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">50</span>
           <span class="s">&quot;pods&quot;</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">50</span>
  <span class="s">&quot;RemovePodsHavingTooManyRestarts&quot;</span><span class="p p-Indicator">:</span>
     <span class="nt">enabled</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">true</span>
     <span class="nt">params</span><span class="p">:</span>
       <span class="nt">podsHavingTooManyRestarts</span><span class="p">:</span>
         <span class="nt">podRestartThresholds</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">100</span>
         <span class="nt">includingInitContainers</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">true</span>
</code></pre></div>

<p><strong>Cluster Turndown</strong></p>
<p><a href="https://github.com/kubecost/cluster-turndown">Cluster Turndown</a> is an automated scaledown and scaleup of a Kubernetes cluster's backing nodes based on a custom schedule and turndown criteria. This feature can be used to reduce spend during down hours and/or reduce surface area for security reasons. The most common use case is to scale non-prod environments (e.g. dev clusters) to zero during off hours. Cluster Turndown is currently in ALPHA release.</p>
<p>Cluster Turndown uses a Kubernetes Custom Resource Definition to create schedules. The following schedule will create a schedule that starts by turning down at the designated start date-time and turning back up at the designated end date-time (times should be in RFC3339 format, i.e. times based on offsets to UTC).</p>
<div class="codehilite"><pre><span></span><code><span class="nt">apiVersion</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">kubecost.k8s.io/v1alpha1</span>
<span class="nt">kind</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">TurndownSchedule</span>
<span class="nt">metadata</span><span class="p">:</span>
  <span class="nt">name</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">example-schedule</span>
  <span class="nt">finalizers</span><span class="p">:</span>
  <span class="p p-Indicator">-</span> <span class="s">&quot;finalizer.kubecost.k8s.io&quot;</span>
<span class="nt">spec</span><span class="p">:</span>
  <span class="nt">start</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">2020-03-12T00:00:00Z</span>
  <span class="nt">end</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">2020-03-12T12:00:00Z</span>
  <span class="nt">repeat</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">daily</span>
</code></pre></div>

<h3 id="use-limitranges-and-resource-quotas-to-help-manage-costs-by-constraining-the-amount-of-resources-allocated-at-an-namespace-level">Use LimitRanges and Resource Quotas to help manage costs by constraining the amount of resources allocated at an Namespace level<a class="headerlink" href="#use-limitranges-and-resource-quotas-to-help-manage-costs-by-constraining-the-amount-of-resources-allocated-at-an-namespace-level" title="Permanent link">&para;</a></h3>
<p>By default, containers run with unbounded compute resources on a Kubernetes cluster. With resource quotas, cluster administrators can restrict resource consumption and creation on a namespace basis. Within a namespace, a Pod or Container can consume as much CPU and memory as defined by the namespace’s resource quota. There is a concern that one Pod or Container could monopolize all available resources. </p>
<p>Kubernetes controls the allocation of resources such as CPU, memory, PersistentVolumeClaims and others using Resource Quotas and Limit Ranges. ResourceQuota is at the Namespace level, while a LimitRange applies at an container level. </p>
<p><strong><em>Limit Ranges</em></strong></p>
<p>A LimitRange is a policy to constrain resource allocations (to Pods or Containers) in a namespace. </p>
<p>The following is an example of setting an default memory request and a default memory limit using Limit Range. </p>
<div class="codehilite"><pre><span></span><code><span class="nt">apiVersion</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">v1</span>
<span class="nt">kind</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">LimitRange</span>
<span class="nt">metadata</span><span class="p">:</span>
  <span class="nt">name</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">mem-limit-range</span>
<span class="nt">spec</span><span class="p">:</span>
  <span class="nt">limits</span><span class="p">:</span>
  <span class="p p-Indicator">-</span> <span class="nt">default</span><span class="p">:</span>
      <span class="nt">memory</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">512Mi</span>
    <span class="nt">defaultRequest</span><span class="p">:</span>
      <span class="nt">memory</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">256Mi</span>
    <span class="nt">type</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">Container</span>
</code></pre></div>

<p>More examples are available in the <a href="https://kubernetes.io/docs/tasks/administer-cluster/manage-resources/memory-default-namespace/">Kubernetes documentation</a>.</p>
<p><strong><em>Resource Quotas</em></strong></p>
<p>When several users or teams share a cluster with a fixed number of nodes, there is a concern that one team could use more than its fair share of resources. Resource quotas are a tool for administrators to address this concern.</p>
<p>The following is an example of how to set quotas for the total amount memory and CPU that can be used by all Containers running in a namespace, by specifying quotas in a ResourceQuota object. This specifies that a Container must have a memory request, memory limit, cpu request, and cpu limit, and should not exceed the threshold set in the ResourceQuota.</p>
<div class="codehilite"><pre><span></span><code><span class="nt">apiVersion</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">v1</span>
<span class="nt">kind</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">ResourceQuota</span>
<span class="nt">metadata</span><span class="p">:</span>
  <span class="nt">name</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">mem-cpu-demo</span>
<span class="nt">spec</span><span class="p">:</span>
  <span class="nt">hard</span><span class="p">:</span>
    <span class="nt">requests.cpu</span><span class="p">:</span> <span class="s">&quot;1&quot;</span>
    <span class="nt">requests.memory</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">1Gi</span>
    <span class="nt">limits.cpu</span><span class="p">:</span> <span class="s">&quot;2&quot;</span>
    <span class="nt">limits.memory</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">2Gi</span>
</code></pre></div>

<p>More examples are available in the <a href="https://kubernetes.io/docs/tasks/administer-cluster/manage-resources/quota-memory-cpu-namespace/">Kubernetes documentation</a>.</p>
<h3 id="use-pricing-models-for-effective-utilization">Use pricing models for effective utilization<a class="headerlink" href="#use-pricing-models-for-effective-utilization" title="Permanent link">&para;</a></h3>
<p>The pricing details for Amazon EKS are given in the <a href="https://aws.amazon.com/eks/pricing/">pricing page</a>. There is a common control plane cost for both Amazon EKS on Fargate and EC2. </p>
<p>If you are using AWS Fargate, pricing is calculated based on the vCPU and memory resources used from the time you start to download your container image until the Amazon EKS pod terminates, rounded up to the nearest second. A minimum charge of 1 minute applies. See detailed pricing information on the <a href="https://aws.amazon.com/fargate/pricing/">AWS Fargate pricing page</a>.</p>
<p><strong><em>Amazon EKS on EC2:</em></strong></p>
<p>Amazon EC2 provides a wide selection of <a href="https://aws.amazon.com/ec2/instance-types/">instance types</a> optimized to fit different use cases. Instance types comprise varying combinations of CPU, memory, storage, and networking capacity and give you the flexibility to choose the appropriate mix of resources for your applications. Each instance type includes one or more instance sizes, allowing you to scale your resources to the requirements of your target workload.</p>
<p>One of the key decision parameters apart from number of CPUs, memory, processor family type related to the instance type is the <a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-eni.html">number of Elastic network interfaces(ENI's)</a>, which in-turn has a bearing on the maximum number of pods you can run on that EC2 Instance. The list of <a href="https://github.com/awslabs/amazon-eks-ami/blob/master/files/eni-max-pods.txt">max pods per EC2 Instance type</a> is maintained in a github.</p>
<p><strong><em>*On-Demand EC2 Instances:</em></strong>*</p>
<p>With <a href="https://aws.amazon.com/ec2/pricing/">On-Demand instances</a>, you pay for compute capacity by the hour or the second depending on which instances you run. No longer-term commitments or upfront payments are needed. </p>
<p>Amazon EC2 A1 instances deliver significant cost savings and are ideally suited for scale-out and ARM-based workloads that are supported by the extensive Arm ecosystem. You can now use Amazon Elastic Container Service for Kubernetes (EKS) to run containers on Amazon EC2 A1 Instances as part of a <a href="https://github.com/aws/containers-roadmap/tree/master/preview-programs/eks-arm-preview">public developer preview</a>. Amazon ECR now supports <a href="https://aws.amazon.com/blogs/containers/introducing-multi-architecture-container-images-for-amazon-ecr/">multi-architecture container images</a>, which makes it simpler to deploy container images for different architectures and operating systems from the same image repository. </p>
<p>You can use the <a href="https://calculator.s3.amazonaws.com/index.html">AWS Simple Monthly Calculator</a> or the new <a href="https://calculator.aws/">pricing calculator</a> to get pricing for the On-Demand EC2 instances for the EKS workder nodes.</p>
<h3 id="use-spot-ec2-instances">Use Spot EC2 Instances:<a class="headerlink" href="#use-spot-ec2-instances" title="Permanent link">&para;</a></h3>
<p>Amazon <a href="https://aws.amazon.com/ec2/pricing/">EC2 Spot instances</a> allow you to request spare Amazon EC2 computing capacity for up to 90% off the On-Demand price. </p>
<p>Spot Instances are often a great fit for stateless containerized workloads because the approach to containers and Spot Instances are similar; ephemeral and autoscaled capacity. This means they both can be added and removed while adhering to SLAs and without impacting the performance or availability of your applications.</p>
<p>You can create multiple nodegroups with a mix of on-demand instance types and EC2 Spot instances to leverage the advantages of pricing between these two instance types.</p>
<p><img alt="On-Demand and Spot Node Groups" src="../../images/spot_diagram.png" />
<strong><em>(Image source: https://ec2spotworkshops.com/using_ec2_spot_instances_with_eks/spotworkers/workers_eksctl.html)</em></strong></p>
<p>A sample yaml file for eksctl to create a nodegroup with EC2 spot instances is given below. During the creation of the Node Group, we have configured a node-label so that kubernetes knows what type of nodes we have provisioned. We set the lifecycle for the nodes as Ec2Spot. We are also tainting with PreferNoSchedule to prefer pods not be scheduled on Spot Instances. This is a “preference” or “soft” version of NoSchedule, i.e. the system will try to avoid placing a pod that does not tolerate the taint on the node, but it is not required. We are using this technique to make sure that only the right type of workloads are scheduled on Spot Instances.</p>
<div class="codehilite"><pre><span></span><code><span class="nt">apiVersion</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">eksctl.io/v1alpha5</span>
<span class="nt">kind</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">ClusterConfig</span>
<span class="nt">metadata</span><span class="p">:</span>
  <span class="nt">name</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">my-cluster-testscaling</span> 
  <span class="nt">region</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">us-west-2</span>
<span class="nt">nodeGroups</span><span class="p">:</span>
  <span class="p p-Indicator">-</span> <span class="nt">name</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">ng-spot</span>
    <span class="nt">labels</span><span class="p">:</span>
      <span class="nt">lifecycle</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">Ec2Spot</span>
    <span class="nt">taints</span><span class="p">:</span>
      <span class="nt">spotInstance</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">true:PreferNoSchedule</span>
    <span class="nt">minSize</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">2</span>
    <span class="nt">maxSize</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">5</span>
    <span class="nt">instancesDistribution</span><span class="p">:</span> <span class="c1"># At least two instance types should be specified</span>
      <span class="nt">instanceTypes</span><span class="p">:</span>
        <span class="p p-Indicator">-</span> <span class="l l-Scalar l-Scalar-Plain">m4.large</span>
        <span class="p p-Indicator">-</span> <span class="l l-Scalar l-Scalar-Plain">c4.large</span>
        <span class="p p-Indicator">-</span> <span class="l l-Scalar l-Scalar-Plain">c5.large</span>
      <span class="nt">onDemandBaseCapacity</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">0</span>
      <span class="nt">onDemandPercentageAboveBaseCapacity</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">0</span> <span class="c1"># all the instances will be spot instances</span>
      <span class="nt">spotInstancePools</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">2</span>
</code></pre></div>

<p>Use the node-labels to identify the lifecycle of the nodes.</p>
<div class="codehilite"><pre><span></span><code>$ kubectl get nodes --label-columns<span class="o">=</span>lifecycle --selector<span class="o">=</span><span class="nv">lifecycle</span><span class="o">=</span>Ec2Spot
</code></pre></div>

<p>We should also deploy the <a href="https://github.com/aws/aws-node-termination-handler">AWS Node Termination Handler</a> on each Spot Instance. This will monitor the EC2 metadata service on the instance for an interruption notice. The termination handler consists of a ServiceAccount, ClusterRole, ClusterRoleBinding, and a DaemonSet. AWS Node Termination Handler is not only for Spot Instances, it can also catch general EC2 maintenance events, so it can be used across all the worker nodes in the cluster.</p>
<p>If a customer is well diversified and uses the capacity-optimized allocation strategy, Spot Instances will be available. You can use Node Affinity in your manifest file to configure this, to prefer Spot Instances, but not require them. This would allow the pods to be scheduled on On-Demand nodes if no spot instances were available or correctly labelled.</p>
<div class="codehilite"><pre><span></span><code><span class="nt">affinity</span><span class="p">:</span>
<span class="nt">nodeAffinity</span><span class="p">:</span>
  <span class="nt">preferredDuringSchedulingIgnoredDuringExecution</span><span class="p">:</span>
  <span class="p p-Indicator">-</span> <span class="nt">weight</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">1</span>
    <span class="nt">preference</span><span class="p">:</span>
      <span class="nt">matchExpressions</span><span class="p">:</span>
      <span class="p p-Indicator">-</span> <span class="nt">key</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">lifecycle</span>
        <span class="nt">operator</span><span class="p">:</span> <span class="l l-Scalar l-Scalar-Plain">In</span>
        <span class="nt">values</span><span class="p">:</span>
        <span class="p p-Indicator">-</span> <span class="l l-Scalar l-Scalar-Plain">Ec2Spot</span>
<span class="nt">tolerations</span><span class="p">:</span>
<span class="p p-Indicator">-</span> <span class="nt">key</span><span class="p">:</span> <span class="s">&quot;spotInstance&quot;</span>
<span class="nt">operator</span><span class="p">:</span> <span class="s">&quot;Equal&quot;</span>
<span class="nt">value</span><span class="p">:</span> <span class="s">&quot;true&quot;</span>
<span class="nt">effect</span><span class="p">:</span> <span class="s">&quot;PreferNoSchedule&quot;</span>
</code></pre></div>

<p>You can do a complete workshop with EC2 spot instances at the <a href="https://ec2spotworkshops.com/using_ec2_spot_instances_with_eks.html">online EC2 Spot Workshop</a>.</p>
<h3 id="use-compute-savings-plan">Use Compute Savings Plan<a class="headerlink" href="#use-compute-savings-plan" title="Permanent link">&para;</a></h3>
<p>Compute Savings Plans, a flexible discount model that provides you with the same discounts as Reserved Instances, in exchange for a commitment to use a specific amount (measured in dollars per hour) of compute power over a one or three year period. The details are covered in the <a href="https://aws.amazon.com/savingsplans/faq/">Savings Plan launch FAQ</a>.The plans automatically apply to any EC2 worker node regardless of region, instance family, operating system, or tenancy, including those that are part of EKS clusters. For example, you can shift from C4 to C5 instances, move a workload from Dublin to London benefiting from Savings Plan prices along the way, without having to do anything.</p>
<p>The AWS Cost Explorer will help you to choose a Savings Plan, and will guide you through the purchase process.
<img alt="Compute Savings Plan" src="../../images/Compute-savings-plan.png" /></p>
<p>Note - The compute savings plans now also applies to <a href="https://aws.amazon.com/about-aws/whats-new/2020/08/amazon-fargate-aws-eks-included-compute-savings-plan/">AWS Fargate for AWS Elastic Kubernetes Service (EKS)</a>. </p>
<p>Note - The above pricing does not include the other AWS services like Data transfer charges, CloudWatch, Elastic Load Balancer and other AWS services that may be used by the Kubernetes applications.</p>
<h2 id="resources">Resources<a class="headerlink" href="#resources" title="Permanent link">&para;</a></h2>
<p>Refer to the following resources to learn more about best practices for cost optimization.</p>
<h3 id="videos">Videos<a class="headerlink" href="#videos" title="Permanent link">&para;</a></h3>
<ul>
<li><a href="https://www.youtube.com/watch?v=7q5AeoKsGJw">AWS re:Invent 2019: Save up to 90% and run production workloads on Spot Instances (CMP331-R1)</a></li>
</ul>
<h3 id="documentation-and-blogs">Documentation and Blogs<a class="headerlink" href="#documentation-and-blogs" title="Permanent link">&para;</a></h3>
<ul>
<li><a href="https://aws.amazon.com/blogs/containers/cost-optimization-for-kubernetes-on-aws/">Cost optimization for Kubernetes on AWS</a></li>
<li><a href="https://aws.amazon.com/blogs/compute/cost-optimization-and-resilience-eks-with-spot-instances/">Building for Cost optimization and Resilience for EKS with Spot Instances</a></li>
<li><a href="https://aws.amazon.com/blogs/containers/autoscaling-eks-on-fargate-with-custom-metrics/">Autoscaling EKS on Fargate with custom metrics</a></li>
<li><a href="https://docs.aws.amazon.com/eks/latest/userguide/fargate.html">AWS Fargate considerations</a></li>
<li><a href="https://ec2spotworkshops.com/using_ec2_spot_instances_with_eks.html">Using Spot Instances with EKS</a></li>
<li><a href="https://aws.amazon.com/blogs/containers/eks-managed-node-groups/">Extending the EKS API: Managed Node Groups</a></li>
<li><a href="https://docs.aws.amazon.com/eks/latest/userguide/autoscaling.html">Autoscaling with Amazon EKS</a> </li>
<li><a href="https://aws.amazon.com/eks/pricing/">Amazon EKS pricing</a></li>
<li><a href="https://aws.amazon.com/fargate/pricing/">AWS Fargate pricing</a></li>
<li><a href="https://docs.aws.amazon.com/savingsplans/latest/userguide/what-is-savings-plans.html">Savings Plan</a></li>
<li><a href="https://srcco.de/posts/saving-cloud-costs-kubernetes-aws.html">Saving Cloud Costs with Kubernetes on AWS</a> </li>
</ul>
<h3 id="tools">Tools<a class="headerlink" href="#tools" title="Permanent link">&para;</a></h3>
<ul>
<li><a href="https://github.com/hjacobs/kube-downscaler">Kube downscaler</a></li>
<li><a href="https://github.com/kubernetes-sigs/descheduler">Kubernetes Descheduler</a></li>
<li><a href="https://github.com/kubecost/cluster-turndown">Cluster TurnDown</a></li>
</ul>
                
                  
                
              
              
                


              
            </article>
          </div>
        </div>
        
      </main>
      
        
<footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
        Made with
        <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
          Material for MkDocs
        </a>
        
      </div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    <script id="__config" type="application/json">{"base": "../..", "features": ["tabs"], "translations": {"clipboard.copy": "Copy to clipboard", "clipboard.copied": "Copied to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.placeholder": "Search", "search.result.placeholder": "Type to start searching", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.term.missing": "Missing"}, "search": "../../assets/javascripts/workers/search.fe42c31b.min.js", "version": null}</script>
    
    
      <script src="../../assets/javascripts/bundle.4ea5477f.min.js"></script>
      
    
  </body>
</html>